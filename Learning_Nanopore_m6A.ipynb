{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning m6A RNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import h5py\n",
    "import random\n",
    "from tombo import tombo_helper\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast5 file paths\n",
    "fast5s = {'unm': '/Volumes/DATA_3/190926_DrKuroyanagi_sams_345/190926_DrKuroyanagi_sams_345/20190926_0543_MN17290_FAK89523_0ee12f5a/fast5_pass_single_unm',\n",
    "          'm6A': '/Volumes/DATA_3/190926_DrKuroyanagi_sams_345/190926_DrKuroyanagi_sams_345/20190926_0543_MN17290_FAK89523_0ee12f5a/fast5_pass_single_m6A',\n",
    "          'vivo': '/Volumes/DATA_2/sams/MinION/20190326_0351_190326_DrKuroyanagi_w12457/RAW/fast5',\n",
    "          'unm long': '/Volumes/DATA_2/sams/MinION/20190326_0350_190326_DrKuroyanagi_sams3_sams4/RAW/fast5_filtered'\n",
    "          }\n",
    "\n",
    "# range\n",
    "region = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m6A sites\n",
    "plotSites = {'sams-3b': 1262, 'sams-3c': 1538, 'sams-4b': 1265, 'sams-4c': 1388, 'sams-4d': 1545, 'sams-5b': 1315}\n",
    "\n",
    "# tombo groups\n",
    "tombo_groups = {'unm': 'RawGenomeCorrected_sams3b5',\n",
    "                'm6A': 'RawGenomeCorrected_sams3b5',\n",
    "                'vivo': 'RawGenomeCorrected_sams3abc4bdef5',\n",
    "                'unm long': 'RawGenomeCorrected_sams3abc4bdef5'\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07:00:38] Parsing Tombo index file(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-3b in unm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='reads', max=7954, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-3c in unm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='reads', max=1, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-4b in unm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='reads', max=1, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-4c in unm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='reads', max=1, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-4d in unm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='reads', max=1, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-5b in unm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='reads', max=13736, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07:03:23] Parsing Tombo index file(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-3b in m6A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='reads', max=3295, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-3c in m6A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='reads', max=1, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-4b in m6A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='reads', max=1, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-4c in m6A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='reads', max=1, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-4d in m6A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='reads', max=1, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-5b in m6A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='reads', max=5376, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07:05:09] Parsing Tombo index file(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-3b in vivo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='reads', max=110, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-3c in vivo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='reads', max=29, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-4b in vivo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='reads', max=49, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-4c in vivo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='reads', max=58, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-4d in vivo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='reads', max=32, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-5b in vivo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='reads', max=18, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07:05:26] Parsing Tombo index file(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-3b in unm long\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='reads', max=128267, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-3c in unm long\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='reads', max=58852, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-4b in unm long\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='reads', max=53515, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-4c in unm long\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='reads', max=53504, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-4d in unm long\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='reads', max=59187, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sams-5b in unm long\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='reads', max=5, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract current data\n",
    "\n",
    "# each sample\n",
    "# container\n",
    "current = pd.DataFrame()\n",
    "for sample_name, fast5_path in fast5s.items():\n",
    "    \n",
    "    # load tombo-annotated reads\n",
    "    tmb = tombo_helper.TomboReads([fast5_path], corrected_group=tombo_groups[sample_name])\n",
    "    \n",
    "    \n",
    "    # each transcript\n",
    "    df2 = pd.DataFrame()\n",
    "    for transcript, position in plotSites.items():\n",
    "        \n",
    "        # print now\n",
    "        print(transcript + ' in ' + sample_name)\n",
    "        \n",
    "        # get reads on a sams gene\n",
    "        reads = tmb.get_cs_reads(chrm=transcript, strand='+')\n",
    "        \n",
    "        # shuffle reads\n",
    "        random.shuffle(reads)\n",
    "\n",
    "\n",
    "        # each read\n",
    "        df1 = []\n",
    "        for read in tqdm(reads, desc='reads', leave=False):\n",
    "            \n",
    "            # target region\n",
    "            plotStart = position - read.start - region - 1\n",
    "            plotEnd = position - read.start + region\n",
    "            \n",
    "            # check read length\n",
    "            if plotStart < 0 or read.end < (position + region):\n",
    "                \n",
    "                continue\n",
    "                \n",
    "            # get current data\n",
    "            path = read.fn\n",
    "            f5 = h5py.File(path, 'r')\n",
    "            \n",
    "            mean = f5['Analyses/' + tombo_groups[sample_name] + '/BaseCalled_template/Events'].value['norm_mean'][plotStart:plotEnd]\n",
    "            stdev = f5['Analyses/' + tombo_groups[sample_name] + '/BaseCalled_template/Events'].value['norm_stdev'][plotStart:plotEnd]\n",
    "            duration = f5['Analyses/' + tombo_groups[sample_name] + '/BaseCalled_template/Events'].value['length'][plotStart:plotEnd]\n",
    "            \n",
    "            df1.append(np.concatenate([mean, stdev, duration]))\n",
    "            \n",
    "            \n",
    "        df1 = pd.DataFrame(df1)\n",
    "\n",
    "        # label transcript\n",
    "        df1['sams'] = transcript\n",
    "        \n",
    "        df2 = df2.append(df1)\n",
    "        \n",
    "    \n",
    "    # label sample\n",
    "    df2['RNA'] = sample_name\n",
    "    \n",
    "    current = current.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(373772, 305)\n"
     ]
    }
   ],
   "source": [
    "# save current data\n",
    "with open('fast5_current_m6A_sams-345_100nt.pickle', 'wb') as f:\n",
    "    pickle.dump(current, f)\n",
    "print(current.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save csv\n",
    "current.to_csv('fast5_current_m6A_sams-345_100nt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Nanopore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Tuning by TPE\n",
    "from hyperopt import hp, tpe, Trials, fmin, space_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(373772, 305)\n"
     ]
    }
   ],
   "source": [
    "# load current data\n",
    "with open('fast5_current_m6A_sams-345_100nt.pickle', 'rb') as f:\n",
    "    current = pickle.load(f)\n",
    "print(current.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5443, 303)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data set for sams-3b+5b\n",
    "# set range\n",
    "length = range(0,303)\n",
    "df = current[((current['sams'] == 'sams-3b') | (current['sams'] == 'sams-5b')) & ((current['RNA'] == 'unm') | (current['RNA'] == 'm6A'))]\n",
    "\n",
    "# unm or m6A\n",
    "Y = df['RNA']\n",
    "Y = Y.str.replace('unm','0').str.replace('m6A','1').values\n",
    "Y = np.array(list(map(int, Y)))\n",
    "\n",
    "# current\n",
    "X = df.iloc[:,length].values\n",
    "\n",
    "# training set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0, test_size=0.2)\n",
    "\n",
    "# downsampling\n",
    "X_train_m6A = X_train[np.where(Y_train == 1)]\n",
    "X_train_unm = X_train[np.where(Y_train == 0)]\n",
    "X_train_unm = X_train_unm[np.random.choice(len(X_train_unm), len(X_train_m6A), replace=False)]\n",
    "X_train = np.concatenate([X_train_unm, X_train_m6A])\n",
    "Y_train = np.concatenate([np.zeros(len(X_train_m6A), dtype=int), np.ones(len(X_train_m6A), dtype=int)])\n",
    "\n",
    "# size\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set for unm long\n",
    "X_unmlong3b = current[(current['sams'] == 'sams-3b') & (current['RNA'] == 'unm long')].iloc[:,length].values\n",
    "X_unmlong3c = current[(current['sams'] == 'sams-3c') & (current['RNA'] == 'unm long')].iloc[:,length].values\n",
    "X_unmlong4b = current[(current['sams'] == 'sams-4b') & (current['RNA'] == 'unm long')].iloc[:,length].values\n",
    "X_unmlong4c = current[(current['sams'] == 'sams-4c') & (current['RNA'] == 'unm long')].iloc[:,length].values\n",
    "X_unmlong4d = current[(current['sams'] == 'sams-4d') & (current['RNA'] == 'unm long')].iloc[:,length].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data set for vivo\n",
    "X_vivo3b = current[(current['sams'] == 'sams-3b') & (current['RNA'] == 'vivo')].iloc[:,length].values\n",
    "X_vivo3c = current[(current['sams'] == 'sams-3c') & (current['RNA'] == 'vivo')].iloc[:,length].values\n",
    "X_vivo4b = current[(current['sams'] == 'sams-4b') & (current['RNA'] == 'vivo')].iloc[:,length].values\n",
    "X_vivo4c = current[(current['sams'] == 'sams-4c') & (current['RNA'] == 'vivo')].iloc[:,length].values\n",
    "X_vivo4d = current[(current['sams'] == 'sams-4d') & (current['RNA'] == 'vivo')].iloc[:,length].values\n",
    "X_vivo5b = current[(current['sams'] == 'sams-5b') & (current['RNA'] == 'vivo')].iloc[:,length].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mkl\n",
    "mkl.set_num_threads(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuroyanagi/anaconda3/envs/keras/lib/python3.6/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "# Decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# SVC\n",
    "from sklearn.svm import SVC\n",
    "# AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# GaussianNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# QuadraticDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "# MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "# LightGBM\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare classifiers by tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting\n",
    "name = 'GradientBoostingClassifier'\n",
    "classifier = GradientBoostingClassifier\n",
    "params = {'learning_rate' : hp.uniform('learning_rate', 0.01, 1),\n",
    "          'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "          'min_samples_leaf': hp.choice('min_samples_leaf', range(1,20)),\n",
    "          'max_features': hp.uniform('max_features', 0.01, 1)\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "name = 'XGBoost'\n",
    "classifier = xgb.XGBClassifier\n",
    "params = {\n",
    "            'learning_rate':    hp.uniform('learning_rate', 0.01, 1),\n",
    "            'max_depth':        hp.choice('max_depth', np.arange(1, 20, 1, dtype=int)),\n",
    "            'min_child_weight': hp.choice('min_child_weight', np.arange(1, 10, 1, dtype=int)),\n",
    "            'colsample_bytree': hp.uniform('colsample_bytree', 0.2, 1),\n",
    "            'subsample':        hp.uniform('subsample', 0.2, 1),\n",
    "            'n_estimators':     100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "name = 'LightGBM'\n",
    "classifier = lgbm.LGBMClassifier\n",
    "params = {\n",
    "            'learning_rate':    hp.uniform('learning_rate', 0.01, 1),\n",
    "            'max_depth':        hp.choice('max_depth', np.arange(1, 20, 1, dtype=int)),\n",
    "            'min_child_weight': hp.choice('min_child_weight', np.arange(1, 10, 1, dtype=int)),\n",
    "            'colsample_bytree': hp.uniform('colsample_bytree', 0.2, 1),\n",
    "            'subsample':        hp.uniform('subsample', 0.2, 1),\n",
    "            'n_estimators':     100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree\n",
    "name = 'DecisionTree'\n",
    "classifier = DecisionTreeClassifier\n",
    "params = {\n",
    "            'max_depth': hp.choice('max_depth', np.arange(1, 20, 1, dtype=int)),\n",
    "            'max_features': hp.choice('max_features', np.arange(1, 20, 1, dtype=int)),\n",
    "            'min_samples_split': hp.choice('min_samples_split', np.arange(2, 20, 1, dtype=int)),\n",
    "            'min_samples_leaf': hp.choice('min_samples_leaf', np.arange(1, 20, 1, dtype=int))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "name = 'RandomForest'\n",
    "classifier = RandomForestClassifier\n",
    "params = {\n",
    "            'max_depth': hp.choice('max_depth', np.arange(1, 20, 1, dtype=int)),\n",
    "            'max_features': hp.choice('max_features', np.arange(1, 20, 1, dtype=int)),\n",
    "            'min_samples_split': hp.choice('min_samples_split', np.arange(2, 20, 1, dtype=int)),\n",
    "            'min_samples_leaf': hp.choice('min_samples_leaf', np.arange(1, 20, 1, dtype=int))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting parameters\n",
    "# file name\n",
    "base_name = 'Hyperopt_' + name + '_m6A_Nanopore_current_100nt_sams-3b5b'\n",
    "\n",
    "# function to minimize\n",
    "def objective(args):\n",
    "    clf = classifier(**args)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    scoreTest = clf.score(X_test,Y_test)\n",
    "    return -1*scoreTest\n",
    "\n",
    "# save steps\n",
    "trials = Trials()\n",
    "\n",
    "# tuning\n",
    "best = fmin(\n",
    "    objective,\n",
    "    params,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# best params\n",
    "clf = classifier(**space_eval(params, best))\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# save model\n",
    "with open(base_name + '.pickle', 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "    \n",
    "    \n",
    "# output scores\n",
    "out_path = base_name + '.txt'\n",
    "with open(out_path, mode='w') as f:\n",
    "\n",
    "    \n",
    "    # training set\n",
    "    accuracyTrain = clf.score(X_train,Y_train)\n",
    "    \n",
    "    \n",
    "    # test set\n",
    "    accuracyTest = clf.score(X_test,Y_test)\n",
    "    \n",
    "    predictTestUnm = clf.predict(X_test[np.where(Y_test == 0)])\n",
    "    scoreTestUnm = len(predictTestUnm[predictTestUnm == 1])/len(predictTestUnm)\n",
    "    \n",
    "    predictTestm6A = clf.predict(X_test[np.where(Y_test == 1)])\n",
    "    scoreTestm6A = len(predictTestm6A[predictTestm6A == 1])/len(predictTestm6A)\n",
    "    \n",
    "    \n",
    "    # unmodified long\n",
    "    scoreUnmlong3b = np.count_nonzero(clf.predict(X_unmlong3b))/len(X_unmlong3b)\n",
    "    scoreUnmlong3c = np.count_nonzero(clf.predict(X_unmlong3c))/len(X_unmlong3c)\n",
    "    scoreUnmlong4b = np.count_nonzero(clf.predict(X_unmlong4b))/len(X_unmlong4b)\n",
    "    scoreUnmlong4c = np.count_nonzero(clf.predict(X_unmlong4c))/len(X_unmlong4c)\n",
    "    scoreUnmlong4d = np.count_nonzero(clf.predict(X_unmlong4d))/len(X_unmlong4d)\n",
    "\n",
    "\n",
    "    # vivo\n",
    "    scoreVivo3b = np.count_nonzero(clf.predict(X_vivo3b))/len(X_vivo3b)\n",
    "    scoreVivo3c = np.count_nonzero(clf.predict(X_vivo3c))/len(X_vivo3c)\n",
    "    scoreVivo4b = np.count_nonzero(clf.predict(X_vivo4b))/len(X_vivo4b)\n",
    "    scoreVivo4c = np.count_nonzero(clf.predict(X_vivo4c))/len(X_vivo4c)\n",
    "    scoreVivo4d = np.count_nonzero(clf.predict(X_vivo4d))/len(X_vivo4d)\n",
    "    scoreVivo5b = np.count_nonzero(clf.predict(X_vivo5b))/len(X_vivo5b)\n",
    "    \n",
    "    \n",
    "    # write results\n",
    "    label = 'accuracyTrain,accuracyTest,scoreTestUnm,scoreTestm6A,scoreUnmlong3b,scoreUnmlong3c,'\\\n",
    "            'scoreUnmlong4b,scoreUnmlong4c,scoreUnmlong4d,scoreVivo3b,scoreVivo3c,scoreVivo4b,scoreVivo4c,scoreVivo4d,scoreVivo5b'\n",
    "    \n",
    "    f.write('Name,%s\\n'\\\n",
    "            \n",
    "            'Size,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s\\n'\\\n",
    "            \n",
    "            'Score,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s' \n",
    "            \n",
    "            % (label,\n",
    "               \n",
    "               len(X_train), len(X_test), len(predictTestUnm), len(predictTestm6A),\n",
    "               len(X_unmlong3b), len(X_unmlong3c), len(X_unmlong4b), len(X_unmlong4c), len(X_unmlong4d),\n",
    "               len(X_vivo3b), len(X_vivo3c), len(X_vivo4b), len(X_vivo4c), len(X_vivo4d), len(X_vivo5b),\n",
    "               \n",
    "               accuracyTrain, accuracyTest, scoreTestUnm, scoreTestm6A,\n",
    "               scoreUnmlong3b, scoreUnmlong3c, scoreUnmlong4b, scoreUnmlong4c, scoreUnmlong4d,\n",
    "               scoreVivo3b, scoreVivo3c, scoreVivo4b, scoreVivo4c, scoreVivo4d, scoreVivo5b\n",
    "            ))\n",
    "\n",
    "# importance from Gradient boosting model\n",
    "if hasattr(clf, 'feature_importances_'):\n",
    "    np.savetxt('Importance_' + base_name + '.csv', clf.feature_importances_, delimiter=',')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare classifiers by tuned parameters and scaled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "stdsc = StandardScaler()\n",
    "X_train = stdsc.fit_transform(X_train)\n",
    "X_test = stdsc.transform(X_test)\n",
    "\n",
    "X_unmlong3b = stdsc.transform(X_unmlong3b)\n",
    "X_unmlong3c = stdsc.transform(X_unmlong3c)\n",
    "X_unmlong4b = stdsc.transform(X_unmlong4b)\n",
    "X_unmlong4c = stdsc.transform(X_unmlong4c)\n",
    "X_unmlong4d = stdsc.transform(X_unmlong4d)\n",
    "\n",
    "X_vivo3b = stdsc.transform(X_vivo3b)\n",
    "X_vivo3c = stdsc.transform(X_vivo3c)\n",
    "X_vivo4b = stdsc.transform(X_vivo4b)\n",
    "X_vivo4c = stdsc.transform(X_vivo4c)\n",
    "X_vivo4d = stdsc.transform(X_vivo4d)\n",
    "X_vivo5b = stdsc.transform(X_vivo5b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM # scaling\n",
    "name = 'SVM'\n",
    "classifier = SVC\n",
    "params = {'C':hp.loguniform('C', -6, 2),\n",
    "          'gamma': hp.loguniform('gamma', -6, 2),\n",
    "          'kernel': hp.choice('kernel', ['linear', 'rbf', 'poly']),\n",
    "          'cache_size': 10000\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression # scaling\n",
    "name = 'LogisticRegression'\n",
    "classifier = LogisticRegression\n",
    "params = {\n",
    "            'C': hp.uniform('C', 0.00001, 1000),\n",
    "            'random_state': hp.choice('random_state', np.arange(1, 100, 1, dtype=int))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN # scaling\n",
    "name = 'KNeighbors'\n",
    "classifier = KNeighborsClassifier\n",
    "params = {\n",
    "            'weights': hp.choice('weights', ['uniform','distance']),\n",
    "            'leaf_size': hp.choice('leaf_size', np.arange(5, 50, 5, dtype=int)),\n",
    "            'n_neighbors': hp.choice('n_neighbors', np.arange(1, 30, 1, dtype=int)),\n",
    "            'p': hp.choice('p', np.arange(1, 3, 1, dtype=int))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLPClassifier # scaling\n",
    "name = 'MLP'\n",
    "classifier = MLPClassifier\n",
    "params = {\n",
    "            'alpha': hp.loguniform('alpha', np.log(0.0001), np.log(0.9)),\n",
    "            'hidden_layer_sizes': hp.choice('hidden_layer_sizes', np.arange(100, 1000, 50, dtype=int)),\n",
    "            'learning_rate': hp.choice('learning_rate', ['constant','adaptive']),\n",
    "            'activation': 'relu',\n",
    "            'solver': 'adam'\n",
    "}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fitting parameters\n",
    "# file name\n",
    "base_name = 'Hyperopt_' + name + '_m6A_Nanopore_current_100nt_sams-3b5b'\n",
    "\n",
    "# function to minimize\n",
    "def objective(args):\n",
    "    clf = classifier(**args)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    scoreTest = clf.score(X_test,Y_test)\n",
    "    return -1*scoreTest\n",
    "\n",
    "# save steps\n",
    "trials = Trials()\n",
    "\n",
    "# tuning\n",
    "best = fmin(\n",
    "    objective,\n",
    "    params,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# best params\n",
    "clf = classifier(**space_eval(params, best))\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# save model\n",
    "with open(base_name + '.pickle', 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "    \n",
    "    \n",
    "# output scores\n",
    "out_path = base_name + '.txt'\n",
    "with open(out_path, mode='w') as f:\n",
    "\n",
    "    \n",
    "    # training set\n",
    "    accuracyTrain = clf.score(X_train,Y_train)\n",
    "    \n",
    "    \n",
    "    # test set\n",
    "    accuracyTest = clf.score(X_test,Y_test)\n",
    "    \n",
    "    predictTestUnm = clf.predict(X_test[np.where(Y_test == 0)])\n",
    "    scoreTestUnm = len(predictTestUnm[predictTestUnm == 1])/len(predictTestUnm)\n",
    "    \n",
    "    predictTestm6A = clf.predict(X_test[np.where(Y_test == 1)])\n",
    "    scoreTestm6A = len(predictTestm6A[predictTestm6A == 1])/len(predictTestm6A)\n",
    "    \n",
    "    \n",
    "    # unmodified long\n",
    "    scoreUnmlong3b = np.count_nonzero(clf.predict(X_unmlong3b))/len(X_unmlong3b)\n",
    "    scoreUnmlong3c = np.count_nonzero(clf.predict(X_unmlong3c))/len(X_unmlong3c)\n",
    "    scoreUnmlong4b = np.count_nonzero(clf.predict(X_unmlong4b))/len(X_unmlong4b)\n",
    "    scoreUnmlong4c = np.count_nonzero(clf.predict(X_unmlong4c))/len(X_unmlong4c)\n",
    "    scoreUnmlong4d = np.count_nonzero(clf.predict(X_unmlong4d))/len(X_unmlong4d)\n",
    "\n",
    "\n",
    "    # vivo\n",
    "    scoreVivo3b = np.count_nonzero(clf.predict(X_vivo3b))/len(X_vivo3b)\n",
    "    scoreVivo3c = np.count_nonzero(clf.predict(X_vivo3c))/len(X_vivo3c)\n",
    "    scoreVivo4b = np.count_nonzero(clf.predict(X_vivo4b))/len(X_vivo4b)\n",
    "    scoreVivo4c = np.count_nonzero(clf.predict(X_vivo4c))/len(X_vivo4c)\n",
    "    scoreVivo4d = np.count_nonzero(clf.predict(X_vivo4d))/len(X_vivo4d)\n",
    "    scoreVivo5b = np.count_nonzero(clf.predict(X_vivo5b))/len(X_vivo5b)\n",
    "    \n",
    "    \n",
    "    # write results\n",
    "    label = 'accuracyTrain,accuracyTest,scoreTestUnm,scoreTestm6A,scoreUnmlong3b,scoreUnmlong3c,'\\\n",
    "            'scoreUnmlong4b,scoreUnmlong4c,scoreUnmlong4d,scoreVivo3b,scoreVivo3c,scoreVivo4b,scoreVivo4c,scoreVivo4d,scoreVivo5b'\n",
    "    \n",
    "    f.write('Name,%s\\n'\\\n",
    "            \n",
    "            'Size,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s\\n'\\\n",
    "            \n",
    "            'Score,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s' \n",
    "            \n",
    "            % (label,\n",
    "               \n",
    "               len(X_train), len(X_test), len(predictTestUnm), len(predictTestm6A),\n",
    "               len(X_unmlong3b), len(X_unmlong3c), len(X_unmlong4b), len(X_unmlong4c), len(X_unmlong4d),\n",
    "               len(X_vivo3b), len(X_vivo3c), len(X_vivo4b), len(X_vivo4c), len(X_vivo4d), len(X_vivo5b),\n",
    "               \n",
    "               accuracyTrain, accuracyTest, scoreTestUnm, scoreTestm6A,\n",
    "               scoreUnmlong3b, scoreUnmlong3c, scoreUnmlong4b, scoreUnmlong4c, scoreUnmlong4d,\n",
    "               scoreVivo3b, scoreVivo3c, scoreVivo4b, scoreVivo4c, scoreVivo4d, scoreVivo5b\n",
    "            ))\n",
    "\n",
    "# importance from Gradient boosting model\n",
    "if hasattr(clf, 'feature_importances_'):\n",
    "    np.savetxt('Importance_' + base_name + '.csv', clf.feature_importances_, delimiter=',')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare classifiers by default parameters and scaled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set classifiers\n",
    "names = [\n",
    "        'Decision Tree',\n",
    "        'Random Forest', \n",
    "        'Logistic Regression',\n",
    "        'K-Nearest Neighbor',\n",
    "        'SVM',\n",
    "        'Adaptive Boosting',\n",
    "        'Gradient Boosting',\n",
    "        'Gaussian Naive Bayes',\n",
    "        'Linear Discriminant Analysis',\n",
    "        'Quadratic Discriminant Analysis',\n",
    "        'Multilayer Perceptron',\n",
    "        'XGBoost',\n",
    "        'LightGBM'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "classifiers = [\n",
    "                DecisionTreeClassifier(),\n",
    "                RandomForestClassifier(),\n",
    "                LogisticRegression(),\n",
    "                KNeighborsClassifier(),\n",
    "                SVC(),\n",
    "                AdaBoostClassifier(),\n",
    "                GradientBoostingClassifier(),\n",
    "                GaussianNB(),\n",
    "                LinearDiscriminantAnalysis(),\n",
    "                QuadraticDiscriminantAnalysis(),\n",
    "                MLPClassifier(),\n",
    "                xgb.XGBClassifier(),\n",
    "                lgbm.LGBMClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuroyanagi/anaconda3/envs/keras/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuroyanagi/anaconda3/envs/keras/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression done\n",
      "K-Nearest Neighbor done\n",
      "SVM done\n",
      "Adaptive Boosting done\n",
      "Gradient Boosting done\n",
      "Gaussian Naive Bayes done\n",
      "Linear Discriminant Analysis done\n",
      "Quadratic Discriminant Analysis done\n",
      "Multilayer Perceptron done\n",
      "XGBoost done\n",
      "LightGBM done\n"
     ]
    }
   ],
   "source": [
    "# compare classifiers\n",
    "out_path = 'sklearn_compareClassifiers_m6A_Nanopore_current_sams-3b5b_100nt.txt'\n",
    "with open(out_path, mode='w') as f:\n",
    "    \n",
    "\n",
    "    # loop classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        \n",
    "        \n",
    "        # fitting\n",
    "        clf = clf.fit(X_train,Y_train)\n",
    "        \n",
    "        \n",
    "        # training set\n",
    "        accuracyTrain = clf.score(X_train,Y_train)\n",
    "\n",
    "\n",
    "        # test set\n",
    "        accuracyTest = clf.score(X_test,Y_test)\n",
    "\n",
    "        predictTestUnm = clf.predict(X_test[np.where(Y_test == 0)])\n",
    "        scoreTestUnm = len(predictTestUnm[predictTestUnm == 1])/len(predictTestUnm)\n",
    "\n",
    "        predictTestm6A = clf.predict(X_test[np.where(Y_test == 1)])\n",
    "        scoreTestm6A = len(predictTestm6A[predictTestm6A == 1])/len(predictTestm6A)\n",
    "\n",
    "\n",
    "        # unmodified long\n",
    "        scoreUnmlong3b = np.count_nonzero(clf.predict(X_unmlong3b))/len(X_unmlong3b)\n",
    "        scoreUnmlong3c = np.count_nonzero(clf.predict(X_unmlong3c))/len(X_unmlong3c)\n",
    "        scoreUnmlong4b = np.count_nonzero(clf.predict(X_unmlong4b))/len(X_unmlong4b)\n",
    "        scoreUnmlong4c = np.count_nonzero(clf.predict(X_unmlong4c))/len(X_unmlong4c)\n",
    "        scoreUnmlong4d = np.count_nonzero(clf.predict(X_unmlong4d))/len(X_unmlong4d)\n",
    "\n",
    "\n",
    "        # vivo\n",
    "        scoreVivo3b = np.count_nonzero(clf.predict(X_vivo3b))/len(X_vivo3b)\n",
    "        scoreVivo3c = np.count_nonzero(clf.predict(X_vivo3c))/len(X_vivo3c)\n",
    "        scoreVivo4b = np.count_nonzero(clf.predict(X_vivo4b))/len(X_vivo4b)\n",
    "        scoreVivo4c = np.count_nonzero(clf.predict(X_vivo4c))/len(X_vivo4c)\n",
    "        scoreVivo4d = np.count_nonzero(clf.predict(X_vivo4d))/len(X_vivo4d)\n",
    "        scoreVivo5b = np.count_nonzero(clf.predict(X_vivo5b))/len(X_vivo5b)\n",
    "\n",
    "\n",
    "        # write results\n",
    "        f.write('%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s\\n' \n",
    "                % (name, accuracyTrain, accuracyTest, scoreTestUnm, scoreTestm6A,\n",
    "                   scoreUnmlong3b, scoreUnmlong3c, scoreUnmlong4b, scoreUnmlong4c, scoreUnmlong4d,\n",
    "                   scoreVivo3b, scoreVivo3c, scoreVivo4b, scoreVivo4c, scoreVivo4d, scoreVivo5b))\n",
    "        \n",
    "        # tracking\n",
    "        print(name + \" done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting\n",
    "name = 'GradientBoostingClassifier'\n",
    "classifier = GradientBoostingClassifier\n",
    "params = {'learning_rate' : hp.uniform('learning_rate', 0.01, 1),\n",
    "          'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "          'min_samples_leaf': hp.choice('min_samples_leaf', range(1,20)),\n",
    "          'max_features': hp.uniform('max_features', 0.01, 1)\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling\n",
    "X_train_m6A = X_train[np.where(Y_train == 1)]\n",
    "X_train_unm = X_train[np.where(Y_train == 0)]\n",
    "X_train_unm = X_train_unm[np.random.choice(len(X_train_unm), len(X_train_m6A), replace=False)]\n",
    "X_train = np.concatenate([X_train_unm, X_train_m6A])\n",
    "Y_train = np.concatenate([np.zeros(len(X_train_m6A), dtype=int), np.ones(len(X_train_m6A), dtype=int)])\n",
    "\n",
    "\n",
    "# function to minimize\n",
    "def objective(args):\n",
    "    clf = classifier(**args)\n",
    "    clf.fit(X_train_down, Y_train_down)\n",
    "    scoreTest = clf.score(X_test,Y_test)\n",
    "    return -1*scoreTest\n",
    "\n",
    "# save steps\n",
    "trials = Trials()\n",
    "\n",
    "# tuning\n",
    "best = fmin(\n",
    "    objective,\n",
    "    params,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=20,\n",
    "    trials=trials,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# best params\n",
    "clf = classifier(**space_eval(params, best))\n",
    "clf.fit(X_train_down, Y_train_down)\n",
    "\n",
    "# save model\n",
    "with open(base_name + '.pickle', 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "    \n",
    "    \n",
    "# output scores\n",
    "out_path = base_name + '.txt'\n",
    "with open(out_path, mode='w') as f:\n",
    "\n",
    "    \n",
    "    # training set\n",
    "    accuracyTrain = clf.score(X_train_down, Y_train_down)\n",
    "    \n",
    "    \n",
    "    # test set\n",
    "    accuracyTest = clf.score(X_test,Y_test)\n",
    "    \n",
    "    predictTestUnm = clf.predict(X_test[np.where(Y_test == 0)])\n",
    "    scoreTestUnm = len(predictTestUnm[predictTestUnm == 1])/len(predictTestUnm)\n",
    "    \n",
    "    predictTestm6A = clf.predict(X_test[np.where(Y_test == 1)])\n",
    "    scoreTestm6A = len(predictTestm6A[predictTestm6A == 1])/len(predictTestm6A)\n",
    "    \n",
    "    \n",
    "    # unmodified long\n",
    "    scoreUnmlong3b = np.count_nonzero(clf.predict(X_unmlong3b))/len(X_unmlong3b)\n",
    "    scoreUnmlong3c = np.count_nonzero(clf.predict(X_unmlong3c))/len(X_unmlong3c)\n",
    "    scoreUnmlong4b = np.count_nonzero(clf.predict(X_unmlong4b))/len(X_unmlong4b)\n",
    "    scoreUnmlong4c = np.count_nonzero(clf.predict(X_unmlong4c))/len(X_unmlong4c)\n",
    "    scoreUnmlong4d = np.count_nonzero(clf.predict(X_unmlong4d))/len(X_unmlong4d)\n",
    "\n",
    "\n",
    "    # vivo\n",
    "    scoreVivo3b = np.count_nonzero(clf.predict(X_vivo3b))/len(X_vivo3b)\n",
    "    scoreVivo3c = np.count_nonzero(clf.predict(X_vivo3c))/len(X_vivo3c)\n",
    "    scoreVivo4b = np.count_nonzero(clf.predict(X_vivo4b))/len(X_vivo4b)\n",
    "    scoreVivo4c = np.count_nonzero(clf.predict(X_vivo4c))/len(X_vivo4c)\n",
    "    scoreVivo4d = np.count_nonzero(clf.predict(X_vivo4d))/len(X_vivo4d)\n",
    "    scoreVivo5b = np.count_nonzero(clf.predict(X_vivo5b))/len(X_vivo5b)\n",
    "    \n",
    "    \n",
    "    # write results\n",
    "    label = 'accuracyTrain,accuracyTest,scoreTestUnm,scoreTestm6A,scoreUnmlong3b,scoreUnmlong3c,'\\\n",
    "            'scoreUnmlong4b,scoreUnmlong4c,scoreUnmlong4d,scoreVivo3b,scoreVivo3c,scoreVivo4b,scoreVivo4c,scoreVivo4d,scoreVivo5b'\n",
    "    \n",
    "    f.write('Name,%s\\n'\\\n",
    "            \n",
    "            'Size,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s\\n'\\\n",
    "            \n",
    "            'Score,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s' \n",
    "            \n",
    "            % (label,\n",
    "               \n",
    "               len(X_train_down), len(X_test), len(predictTestUnm), len(predictTestm6A),\n",
    "               len(X_unmlong3b), len(X_unmlong3c), len(X_unmlong4b), len(X_unmlong4c), len(X_unmlong4d),\n",
    "               len(X_vivo3b), len(X_vivo3c), len(X_vivo4b), len(X_vivo4c), len(X_vivo4d), len(X_vivo5b),\n",
    "               \n",
    "               accuracyTrain, accuracyTest, scoreTestUnm, scoreTestm6A,\n",
    "               scoreUnmlong3b, scoreUnmlong3c, scoreUnmlong4b, scoreUnmlong4c, scoreUnmlong4d,\n",
    "               scoreVivo3b, scoreVivo3c, scoreVivo4b, scoreVivo4c, scoreVivo4d, scoreVivo5b\n",
    "            ))\n",
    "\n",
    "# importance from Gradient boosting model\n",
    "if hasattr(clf, 'feature_importances_'):\n",
    "    np.savetxt('Importance_' + base_name + '.csv', clf.feature_importances_, delimiter=',')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
